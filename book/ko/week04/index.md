# 4주차: 단어 임베딩

## 개요

자연어 처리(NLP)와 대규모 언어 모델(LLM) 과정의 4주차에 오신 것을 환영합니다. 이번 주에는 현대 NLP에서 중요한 개념이며 기계 학습 모델을 위한 단어 표현 방식을 혁신한 단어 임베딩의 흥미로운 세계에 깊이 들어갈 것입니다.

단어 임베딩은 연속적인 벡터 공간에서 의미론적 관계를 포착하는 단어의 밀집 벡터 표현입니다. 전통적인 원-핫 인코딩과 달리, 단어 임베딩은 단어의 의미와 다른 단어와의 관계를 보존하는 방식으로 단어를 표현할 수 있게 해줍니다.

## 학습 목표

이번 주가 끝나면 다음을 할 수 있게 됩니다:

1. 단어 임베딩의 개념과 NLP에서의 중요성 이해
2. Word2Vec, GloVe, FastText 임베딩 모델 간의 주요 차이점 설명
3. Gensim을 사용하여 단어 임베딩 모델 구현 및 학습
4. 의미론적 관계에 대한 통찰을 얻기 위해 단어 임베딩 시각화
5. 다양한 NLP 작업에 사전 학습된 단어 임베딩 적용

## 주요 주제

### 1. 단어 임베딩 소개

- 전통적인 단어 표현(예: 원-핫 인코딩)의 한계
- 분포 의미론의 아이디어
- 단어 임베딩의 특성과 이점

### 2. Word2Vec

- CBOW(Continuous Bag of Words)와 Skip-gram 아키텍처
- 학습 과정과 최적화 기법
- 희귀 단어와 하위 단어 정보 처리

### 3. GloVe (Global Vectors for Word Representation)

- Word2Vec과의 비교
- 동시 출현 행렬과 GloVe가 전역 통계를 활용하는 방법
- 응용 및 사용 사례

### 4. FastText

- 하위 단어 임베딩과 그 장점
- 어휘 외 단어 처리
- 형태학적으로 풍부한 언어에서의 성능

### 5. 단어 임베딩의 실제 응용

- 하위 NLP 작업에서 사전 학습된 임베딩 사용
- 특정 도메인을 위한 임베딩 미세 조정
- 단어 임베딩의 품질 평가

## 실습 구성 요소

이번 주 실습 세션에서는 다음을 수행합니다:

- Gensim 라이브러리 설치 및 설정
- 텍스트 코퍼스에 대한 Word2Vec 모델 학습
- 차원 축소 기법(예: t-SNE)을 사용하여 단어 임베딩 시각화
- 학습된 임베딩을 사용하여 의미론적 관계와 단어 유추 탐색
- 간단한 NLP 작업에서 다양한 임베딩 모델의 성능 비교

## 과제

이번 주 과제는 다음과 같습니다:

1. 제공된 데이터셋에 대해 단어 임베딩 모델(Word2Vec, GloVe, FastText) 학습
2. 학습된 임베딩 시각화 및 분석
3. 학습된 임베딩을 사용하여 간단한 단어 유추 작업 구현
4. 각 임베딩 모델의 성능과 특성을 비교하는 짧은 보고서 작성

## 추천 읽기 자료

- Mikolov, T., 외 (2013). "벡터 공간에서의 단어 표현의 효율적 추정"
- Pennington, J., 외 (2014). "GloVe: 단어 표현을 위한 전역 벡터"
- Bojanowski, P., 외 (2017). "하위 단어 정보로 단어 벡터 강화하기"

```{tableofcontents}

```
