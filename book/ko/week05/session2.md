# 5주차 2세션 - BERT

## 1. BERT 소개

BERT(Bidirectional Encoder Representations from Transformers)는 2018년에 소개되어 자연어 처리(NLP)에 중요한 이정표를 세웠습니다. BERT 논문은 다양한 NLP 작업에서 이전 모델들을 능가하는 새로운 언어 표현 모델을 제시했습니다.

```{image} figs/entelecheia_bert.png
:alt: bert
:class: bg-primary mb-1
:width: 70%
:align: center
```

BERT의 주요 특징:

- 깊은 양방향 트랜스포머 모델입니다
- 레이블이 없는 대규모 텍스트 코퍼스로 사전 훈련됩니다
- 문장 내 마스킹된 단어를 예측하고 다음 문장을 예측하도록 설계되었습니다
- 다양한 하위 NLP 작업에 대해 미세 조정될 수 있습니다

BERT는 두 가지 핵심 아이디어를 기반으로 합니다:

1. 트랜스포머 아키텍처
2. 비지도 사전 학습

BERT의 구조:

- 12 (또는 24) 층
- 12 (또는 16) 어텐션 헤드
- 1억 1천만 개의 매개변수
- 층 간 가중치 공유 없음

## 2. BERT의 아키텍처

### 2.1 어텐션 메커니즘

BERT의 핵심 구성 요소는 어텐션 메커니즘으로, 모델이 특정 작업에 대한 중요도에 따라 입력의 다른 부분에 가중치를 할당할 수 있게 합니다.

예시:
"길 아래에서 온 개가 나에게 달려와서 \_\_\_"라는 문장에서, 문장을 완성하기 위해 모델은 "개"라는 단어에 "길"이라는 단어보다 더 많은 주의를 기울일 수 있습니다.

어텐션 메커니즘은 두 가지 입력을 받습니다:

1. 쿼리: 우리가 집중하고자 하는 입력의 부분
2. 키: 쿼리와 비교하고자 하는 입력의 부분

출력은 키의 값들의 가중 합으로, 가중치는 쿼리와 키의 함수로 계산됩니다.

```{figure} figs/sentence_vector.png
---
width: 80%
name: fig-sentence-vector
---
문장 벡터
```

이 그림은 단어 시퀀스(X)가 벡터로 표현되는 방식을 보여주며, 각 요소(xi)는 차원 d의 값입니다.

```{figure} figs/attention_input_output.png
---
width: 80%
name: fig-sentence-input-output
---
문장 입력 및 출력
```

이 그림은 어텐션이 입력 시퀀스 X를 동일한 길이의 출력 시퀀스 Y로 변환하는 방식을 보여주며, Y는 동일한 차원의 벡터로 구성됩니다.

### 2.2 BERT의 단어 임베딩

BERT는 단어를 의미의 다양한 측면을 포착하는 벡터(단어 임베딩)로 표현합니다.

```{figure} figs/word_embedding.png
---
width: 80%
name: fig-word_embedding
---
단어 임베딩
```

이 그림은 "the dog ran" 문장이 일련의 단어 임베딩 벡터로 표현되는 방식을 보여줍니다.

단어 임베딩을 통해 단어에 대한 산술 연산이 가능합니다:
예시: 고양이 - 새끼고양이 ≈ 개 - 강아지

### 2.3 단순화된 어텐션 메커니즘

```{mermaid}
:align: center

graph TD
A[입력: 고양이가 달렸다] --> B[어텐션 가중치] --> C[출력: 고양이]
```

이 다이어그램은 단순화된 어텐션 메커니즘 버전을 보여주며, 문장에서 중요한 단어에 어떻게 집중하는지 보여줍니다.

```{mermaid}
:align: center

graph LR
A[입력: 고양이가 달렸다] --> B[어텐션 가중치: 0.8, 0.1, 0.1] --> C[출력: 고양이]
```

이 다이어그램은 구체적인 어텐션 가중치를 가진 어텐션 메커니즘을 보여주며, 이 예시에서 모델이 "고양이"라는 단어에 더 집중하는 방식을 보여줍니다.

## 3. 어텐션 분해

### 3.1 쿼리와 키

어텐션은 쿼리와 키를 사용하여 입력의 각 단어에 대한 가중치를 계산합니다.

```{figure} figs/attention_query_key.png
---
width: 80%
name: fig-attention_query_key
---
어텐션 쿼리와 키
```

이 그림은 어텐션 메커니즘에서 쿼리와 키가 어떻게 사용되는지 보여줍니다.

단어 간의 유사성은 쿼리와 키 벡터의 내적을 계산하여 구합니다:

```{figure} figs/attention_query_key_dot_product.png
---
width: 80%
name: fig-attention_query_key_dot_product
---
어텐션 쿼리와 키 내적
```

내적은 소프트맥스 함수를 사용하여 확률로 변환됩니다:

```{figure} figs/attention_query_key_softmax.png
---
width: 80%
name: fig-attention_query_key_softmax
---
어텐션 쿼리와 키 소프트맥스
```

### 3.2 어텐션의 뉴런 관점

뉴런 관점은 어텐션 가중치가 어떻게 계산되는지에 대한 자세한 설명을 제공합니다:

```{figure} figs/neuron_view.png
---
width: 80%
name: fig-neuron_view
---
뉴런 관점
```

이 그림은 어텐션 계산 과정을 분해합니다:

- 쿼리 q: 주의를 기울이는 단어를 인코딩합니다
- 키 k: 주의를 받는 단어를 인코딩합니다
- q×k (요소별): 개별 요소가 내적에 어떻게 기여하는지 보여줍니다
- q·k: 정규화되지 않은 어텐션 점수입니다
- 소프트맥스: 어텐션 점수를 정규화합니다

## 4. 멀티헤드 어텐션

BERT는 동시에 작동하는 여러 어텐션 메커니즘(헤드)을 사용합니다:

- 각 헤드는 고유한 어텐션 메커니즘을 나타냅니다
- 모든 헤드의 출력은 연결되어 피드포워드 신경망에 입력됩니다
- 모델이 단어 간의 다양한 관계를 학습할 수 있게 합니다

## 5. 사전 훈련과 미세 조정

BERT는 두 단계의 훈련 과정을 사용합니다:

### 5.1 사전 훈련

사전 훈련 중에 BERT는 두 가지 비지도 학습 목표를 사용하여 레이블이 없는 대규모 텍스트 데이터 코퍼스에서 훈련됩니다:

1. 마스크 언어 모델링 (MLM):

   - 문장의 일부 단어가 무작위로 마스킹됩니다
   - 모델은 문맥을 기반으로 마스킹된 단어를 예측하도록 훈련됩니다
   - 모델이 단어의 문맥적 표현을 학습하도록 강제합니다

2. 다음 문장 예측 (NSP):
   - 모델에 두 개의 문장이 주어집니다
   - 두 번째 문장이 원본 텍스트에서 첫 번째 문장 다음에 오는지 예측합니다
   - 모델이 문장 간의 관계를 학습하고 장거리 의존성을 포착하는 데 도움을 줍니다

### 5.2 미세 조정

사전 훈련 후, BERT는 레이블이 있는 데이터를 사용하여 특정 NLP 작업에 대해 미세 조정됩니다:

- 모델은 작업별 데이터셋에서 몇 번의 추가 에포크 동안 훈련됩니다
- 사전 훈련된 가중치는 작업별 데이터의 패턴을 더 잘 포착하도록 업데이트됩니다
- BERT가 사전 훈련의 지식을 활용하고 특정 작업에 적응할 수 있게 합니다

이 두 단계 과정을 통해 BERT는 감성 분석, 개체명 인식, 질의응답 등 다양한 NLP 작업에서 최고 수준의 성능을 달성할 수 있습니다.

## 결론

BERT는 NLP에 있어 중요한 발전을 나타내며, 언어 이해에 강력하고 유연한 접근 방식을 제공합니다. 양방향 컨텍스트, 어텐션 메커니즘, 그리고 두 단계 훈련 과정의 사용을 통해 복잡한 언어 패턴을 포착하고 광범위한 NLP 작업에서 우수한 성능을 달성할 수 있습니다.
