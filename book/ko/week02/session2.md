# 2ì£¼ì°¨ ì„¸ì…˜ 2 - ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í‘œí˜„

## ì†Œê°œ

ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê¸°ìˆ ì„ ì‚´í´ë³´ê³  ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤. 1íšŒì°¨ì—ì„œ ë‹¤ë£¬ ê¸°ë³¸ ê°œë…ì„ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•œ ë” ì •êµí•œ ì ‘ê·¼ ë°©ì‹ì„ ì†Œê°œí•  ê²ƒì…ë‹ˆë‹¤.

ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê² ìŠµë‹ˆë‹¤:

```python
import spacy
import re
import nltk
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import spacy
import matplotlib.font_manager as fm
import matplotlib
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec
from konlpy.tag import Okt

# í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# spaCy ëª¨ë¸ ë¡œë“œ
nlp = spacy.load("en_core_web_sm")
```

## 1. ê³ ê¸‰ í…ìŠ¤íŠ¸ ì •ì œ ê¸°ìˆ 

1íšŒì°¨ì˜ ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì •ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ëª‡ ê°€ì§€ ê³ ê¸‰ ê¸°ìˆ ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

### ì´ëª¨ì§€ì™€ ì´ëª¨í‹°ì½˜ ì²˜ë¦¬

ì´ëª¨ì§€ì™€ ì´ëª¨í‹°ì½˜ì€ ê°ì • ì •ë³´ë¥¼ ë‹´ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì„ ì œê±°í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
!pip install emoji
import emoji

def handle_emojis(text):
    # ì´ëª¨ì§€ë¥¼ í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´
    return emoji.demojize(text)

# ì‚¬ìš© ì˜ˆì‹œ
sample_text = "ì´ ì˜í™” ì •ë§ ì¢‹ì•„ìš”! ğŸ˜ğŸ‘"
processed_text = handle_emojis(sample_text)
print("ì›ë³¸ í…ìŠ¤íŠ¸:", sample_text)
print("ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸:", processed_text)
```

### ë¹„ ASCII ë¬¸ì ì²˜ë¦¬

ë¹„ ASCII ë¬¸ìë¥¼ í¬í•¨í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ê²½ìš°, ì´ë¥¼ ì œê±°í•˜ê±°ë‚˜ ì •ê·œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# ìœ ë‹ˆì½”ë“œ ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
import unicodedata

def normalize_unicode(text):
    # ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ê·œí™”: NFKC í˜•íƒœë¡œ ì •ê·œí™” (í•œê¸€ì„ ìœ ì§€)
    return unicodedata.normalize('NFKC', text)

# ì‚¬ìš© ì˜ˆì‹œ
sample_text = "ì¹´í˜ ì˜¤ ë ˆëŠ” ë§›ìˆëŠ” ìŒë£Œì…ë‹ˆë‹¤"
normalized_text = normalize_unicode(sample_text)
print("ì›ë³¸ í…ìŠ¤íŠ¸:", sample_text)
print("ì •ê·œí™”ëœ í…ìŠ¤íŠ¸:", normalized_text)
```

## 2. ì¶•ì•½ì–´ì™€ íŠ¹ìˆ˜ ê²½ìš° ì²˜ë¦¬

ì¶•ì•½ì–´(ì˜ˆ: "don't", "I'm")ëŠ” ì¼ë¶€ NLP ì‘ì—…ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶•ì•½ì–´ë¥¼ í™•ì¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
CONTRACTION_MAP = {
    "ain't": "is not",
    "aren't": "are not",
    "can't": "cannot",
    "don't": "do not",
    "I'm": "I am",
    "isn't": "is not",
    "it's": "it is",
    "won't": "will not",
    # í•„ìš”ì— ë”°ë¼ ë” ë§ì€ ì¶•ì•½ì–´ ì¶”ê°€
}

def expand_contractions(text):
    for contraction, expansion in CONTRACTION_MAP.items():
        text = re.sub(r'\b' + contraction + r'\b', expansion, text, flags=re.IGNORECASE)
    return text

# ì‚¬ìš© ì˜ˆì‹œ
sample_text = "I can't believe it's not butter!"
expanded_text = expand_contractions(sample_text)
print("ì›ë³¸ í…ìŠ¤íŠ¸:", sample_text)
print("í™•ì¥ëœ í…ìŠ¤íŠ¸:", expanded_text)
```

## 3. ê°œì²´ëª… ì¸ì‹ (NER)

ê°œì²´ëª… ì¸ì‹ì€ í…ìŠ¤íŠ¸ì—ì„œ ëª…ëª…ëœ ê°œì²´(ì˜ˆ: ì¸ëª…, ì¡°ì§, ì¥ì†Œ)ë¥¼ ì‹ë³„í•˜ê³  ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. spaCyë¥¼ ì‚¬ìš©í•˜ì—¬ NERì„ ìˆ˜í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
# en_core_web_sm ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤)
!python -m spacy download en_core_web_sm

# NER ëª¨ë¸ ë¡œë“œ (ì˜ì–´ ëª¨ë¸ ì‚¬ìš©)
nlp = spacy.load("en_core_web_sm")

def perform_ner(text):
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

# ì‚¬ìš© ì˜ˆì‹œ
sample_text = "Apple was founded by Steve Jobs in Cupertino, California."
entities = perform_ner(sample_text)
print("ëª…ëª…ëœ ê°œì²´:", entities)

# NER ê²°ê³¼ ì‹œê°í™”
from spacy import displacy
displacy.render(nlp(sample_text), style="ent", jupyter=True)
```

## 4. í’ˆì‚¬ íƒœê¹… (POS Tagging)

í’ˆì‚¬ íƒœê¹…ì€ ë‹¨ì–´ì— ë¬¸ë²•ì  ë²”ì£¼(ì˜ˆ: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)ë¥¼ ë¼ë²¨ë§í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ì‘ì—…ì— NLTKë¥¼ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
# 1. í•œê¸€ í°íŠ¸ ì„¤ì¹˜
!apt-get install -y fonts-nanum

# 2. Colabì—ì„œ ê·¸ë˜í”„ë¥¼ ë°”ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ ë§¤ì§ ëª…ë ¹ì–´ ì¶”ê°€
%matplotlib inline

# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from konlpy.tag import Okt

# ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ê²½ë¡œ ì„¤ì •
font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
font_prop = fm.FontProperties(fname=font_path, size=10)

# í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

# KoNLPyì˜ Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
okt = Okt()

def pos_tag_text(text):
    return okt.pos(text)

# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
sample_text = "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤."
pos_tags = pos_tag_text(sample_text)
print("í’ˆì‚¬ íƒœê·¸:", pos_tags)

# í’ˆì‚¬ íƒœê·¸ ì‹œê°í™”
def plot_pos_tags(pos_tags):
    words, tags = zip(*pos_tags)
    plt.figure(figsize=(12, 6))
    sns.barplot(x=list(words), y=[1]*len(words), hue=list(tags), dodge=False)
    plt.title("í’ˆì‚¬ íƒœê·¸", fontproperties=font_prop)
    plt.xlabel("ë‹¨ì–´", fontproperties=font_prop)
    plt.ylabel("")
    plt.legend(title="í’ˆì‚¬ íƒœê·¸", bbox_to_anchor=(1.05, 1), loc='upper left', prop=font_prop)
    plt.xticks(rotation=45, ha='right')
    for label in plt.gca().get_xticklabels():
        label.set_fontproperties(font_prop)
    plt.tight_layout()
    plt.show() 

# ê·¸ë˜í”„ ì¶œë ¥
plot_pos_tags(pos_tags)
```

## 5. í…ìŠ¤íŠ¸ í‘œí˜„ ë°©ë²•

ì´ì œ ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ìˆ ì„ ë‹¤ë¤˜ìœ¼ë‹ˆ, ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### ë‹¨ì–´ ì£¼ë¨¸ë‹ˆ (Bag of Words, BoW)

ë‹¨ì–´ ì£¼ë¨¸ë‹ˆ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë¹ˆë„ì˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ë©°, ë¬¸ë²•ê³¼ ë‹¨ì–´ ìˆœì„œë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.

```python
def create_bow(corpus):
    vectorizer = CountVectorizer()
    bow_matrix = vectorizer.fit_transform(corpus)
    return bow_matrix, vectorizer.get_feature_names_out()

# ì‚¬ìš© ì˜ˆì‹œ
corpus = [
    "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤.",
    "ê²Œìœ¼ë¥¸ ê°œëŠ” í•˜ë£¨ ì¢…ì¼ ì ì„ ì¡ë‹ˆë‹¤.",
    "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ëŠ” ë¹ ë¦…ë‹ˆë‹¤."
]

bow_matrix, feature_names = create_bow(corpus)
print("BoW íŠ¹ì„± ì´ë¦„:", feature_names)
print("BoW í–‰ë ¬ í˜•íƒœ:", bow_matrix.shape)
print("BoW í–‰ë ¬:")
print(bow_matrix.toarray())

# BoW ì‹œê°í™”
plt.figure(figsize=(12, 6))
sns.heatmap(bow_matrix.toarray(), annot=True, fmt='d', cmap='YlGnBu', xticklabels=feature_names)
plt.title("ë‹¨ì–´ ì£¼ë¨¸ë‹ˆ í‘œí˜„")
plt.ylabel("ë¬¸ì„œ")
plt.xlabel("ë‹¨ì–´")
plt.tight_layout()
plt.show()
```

### TF-IDF (ë‹¨ì–´ ë¹ˆë„-ì—­ë¬¸ì„œ ë¹ˆë„)

TF-IDFëŠ” ë¬¸ì„œ ì§‘í•©ì— ëŒ€í•œ ë¬¸ì„œ ë‚´ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

```python
!apt-get install -y fonts-nanum

# 2. í°íŠ¸ ìºì‹œë¥¼ ì‚­ì œí•˜ì—¬ matplotlibê°€ ë‹¤ì‹œ ìƒì„±í•˜ë„ë¡ ì²˜ë¦¬
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import os

# í°íŠ¸ ê²½ë¡œ ì„¤ì •
font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
fm.fontManager.addfont(font_path)
plt.rc('font', family='NanumGothic')

# ìºì‹œ íŒŒì¼ ì‚­ì œ (matplotlibê°€ ìºì‹œë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ë„ë¡ ê°•ì œ)
font_cache_dir = os.path.expanduser('~/.cache/matplotlib')
if os.path.exists(font_cache_dir):
    import shutil
    shutil.rmtree(font_cache_dir)

# ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€ ì„¤ì •
plt.rcParams['axes.unicode_minus'] = False

# TF-IDF ìƒì„± í•¨ìˆ˜
def create_tfidf(corpus):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(corpus)
    return tfidf_matrix, vectorizer.get_feature_names_out()

# ì‚¬ìš© ì˜ˆì‹œ
corpus = [
    "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤.",
    "ì¢…ì¼ ì ì„ ì¡ë‹ˆë‹¤.",
    "ë¹ ë¥¸ ì—¬ìš°ëŠ” ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤."
]

tfidf_matrix, feature_names = create_tfidf(corpus)
print("TF-IDF íŠ¹ì„± ì´ë¦„:", feature_names)
print("TF-IDF í–‰ë ¬ í˜•íƒœ:", tfidf_matrix.shape)
print("TF-IDF í–‰ë ¬:")
print(tfidf_matrix.toarray())

# TF-IDF ì‹œê°í™”
plt.figure(figsize=(12, 6))
sns.heatmap(tfidf_matrix.toarray(), annot=True, fmt='.2f', cmap='YlGnBu', xticklabels=feature_names)
plt.title("TF-IDF í‘œí˜„")
plt.ylabel("ë¬¸ì„œ")
plt.xlabel("ë‹¨ì–´")
plt.tight_layout()
plt.show()
```

## 6. ë‹¨ì–´ ì„ë² ë”©

ë‹¨ì–´ ì„ë² ë”©ì€ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í¬ì°©í•˜ëŠ” ë‹¨ì–´ì˜ ë°€ì§‘ ë²¡í„° í‘œí˜„ì…ë‹ˆë‹¤. ì¸ê¸° ìˆëŠ” ë‹¨ì–´ ì„ë² ë”© ê¸°ìˆ ì¸ Word2Vecì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

```python
!pip install konlpy
import numpy as np
from konlpy.tag import Okt
from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Word2Vec í•™ìŠµ í•¨ìˆ˜
def train_word2vec(sentences, vector_size=100, window=5, min_count=1):
    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count)
    return model

# í˜•íƒœì†Œ ë¶„ì„ê¸° (Okt) ì´ˆê¸°í™”
okt = Okt()

# í•œêµ­ì–´ ë¬¸ì¥ì„ í† í°í™”í•˜ëŠ” í•¨ìˆ˜
def tokenize_korean(corpus):
    return [okt.morphs(sentence) for sentence in corpus]

# ì‚¬ìš© ì˜ˆì‹œ
corpus = ["ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤.", "ì¢…ì¼ ì ì„ ì¡ë‹ˆë‹¤.", "ë¹ ë¥¸ ì—¬ìš°ëŠ” ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤."]
tokenized_corpus = tokenize_korean(corpus)
w2v_model = train_word2vec(tokenized_corpus)

# ëª¨ë¸ì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def get_model_words(model, words):
    return [word for word in words if word in model.wv]

# t-SNEë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ì„ë² ë”© ì‹œê°í™”
def plot_word_embeddings(model, words, perplexity=2):
    words_in_model = get_model_words(model, words)

    if len(words_in_model) < 2:
        print("ì‹œê°í™”í•  ë‹¨ì–´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # Word vectorsë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    word_vectors = np.array([model.wv[word] for word in words_in_model])

    # perplexity ê°’ì„ ë‹¨ì–´ ìˆ˜ë³´ë‹¤ ì‘ê²Œ ì„¤ì •
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(perplexity, len(word_vectors) - 1))
    embedded = tsne.fit_transform(word_vectors)

    plt.figure(figsize=(12, 8))
    for i, word in enumerate(words_in_model):
        x, y = embedded[i]
        plt.scatter(x, y)
        plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')
    plt.title("ë‹¨ì–´ ì„ë² ë”© ì‹œê°í™”")
    plt.xlabel("t-SNE íŠ¹ì„± 0")
    plt.ylabel("t-SNE íŠ¹ì„± 1")
    plt.tight_layout()
    plt.show()

# ëª¨ë¸ì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ë¡œ ì‹œê°í™”
words_to_visualize = ['ë¹ ë¥¸', 'ê°ˆìƒ‰', 'ì—¬ìš°', 'ê²Œìœ¼ë¥¸', 'ê°œ', 'ë›°ì–´ë„˜ìŠµë‹ˆë‹¤', 'ì ì„']
plot_word_embeddings(w2v_model, words_to_visualize, perplexity=2)

# í•™ìŠµëœ ëª¨ë¸ì—ì„œ ë‹¨ì–´ë“¤ì„ í™•ì¸í•´ë´…ë‹ˆë‹¤
print("ëª¨ë¸ì— í¬í•¨ëœ ë‹¨ì–´ë“¤:", w2v_model.wv.index_to_key)

# ëª¨ë¸ì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ë“¤ë§Œ ì¶”ì¶œí•œ ëª©ë¡ í™•ì¸
words_in_model = get_model_words(w2v_model, words_to_visualize)
print("ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ë“¤:", words_in_model)

# ì‹œê°í™”í•  ë‹¨ì–´ê°€ ì—†ëŠ” ê²½ìš°, ì´ ë¶€ë¶„ì„ í†µí•´ ì›ì¸ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
if len(words_in_model) < 2:
    print("ì‹œê°í™”í•  ë‹¨ì–´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë¸ì— ë‹¨ì–´ê°€ í•™ìŠµë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    plot_word_embeddings(w2v_model, words_to_visualize, perplexity=2)
```

## 7. ê²°ë¡  ë° ëª¨ë²” ì‚¬ë¡€

ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê¸°ìˆ ê³¼ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ëª…ì‹¬í•´ì•¼ í•  ëª‡ ê°€ì§€ ëª¨ë²” ì‚¬ë¡€ì…ë‹ˆë‹¤:

1. íŠ¹ì • ì‘ì—…ê³¼ ë°ì´í„°ì…‹ì— ê¸°ë°˜í•˜ì—¬ ì „ì²˜ë¦¬ ê¸°ìˆ ì„ ì„ íƒí•˜ì„¸ìš”.
2. ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ í‘œí˜„ ë°©ë²• ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”:
   - BoWì™€ TF-IDFëŠ” ê°„ë‹¨í•˜ì§€ë§Œ ì˜ë¯¸ì  ì •ë³´ë¥¼ ìƒì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   - ë‹¨ì–´ ì„ë² ë”©ì€ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í¬ì°©í•˜ì§€ë§Œ íš¨ê³¼ì ìœ¼ë¡œ í›ˆë ¨í•˜ë ¤ë©´ ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.
3. ë‹¨ì–´ ì„ë² ë”©ì„ ì‚¬ìš©í•  ë•ŒëŠ” íŠ¹íˆ ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.
4. íŠ¹íˆ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ë‹¤ë£° ë•Œ ë‹¤ì–‘í•œ í‘œí˜„ ë°©ë²•ì— í•„ìš”í•œ ê³„ì‚° ë¦¬ì†ŒìŠ¤ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.
5. ì „ì²˜ë¦¬ ë° í‘œí˜„ ì„ íƒì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ê¸°ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì™€ í‘œí˜„ì€ NLP íŒŒì´í”„ë¼ì¸ì—ì„œ ì¤‘ìš”í•œ ë‹¨ê³„ì´ë©°, ì—¬ê¸°ì„œ ë‚´ë¦¬ëŠ” ì„ íƒì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒì„ ê¸°ì–µí•˜ì„¸ìš”.

## ì—°ìŠµ

1. ì„ íƒí•œ ë°ì´í„°ì…‹(ì˜ˆ: ë‰´ìŠ¤ ê¸°ì‚¬ ëª¨ìŒ, íŠ¸ìœ—, ì œí’ˆ ë¦¬ë·°)ì„ ì„ íƒí•˜ì„¸ìš”.
2. ìš°ë¦¬ê°€ ë°°ìš´ ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ìˆ (ì˜ˆ: ì´ëª¨ì§€ ì²˜ë¦¬, ì¶•ì•½ì–´ í™•ì¥, NER)ì„ ì ìš©í•˜ì„¸ìš”.
3. ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì˜ BoWì™€ TF-IDF í‘œí˜„ì„ ëª¨ë‘ ë§Œë“œì„¸ìš”.
4. ë°ì´í„°ì…‹ì—ì„œ Word2Vec ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ëª‡ ê°€ì§€ í¥ë¯¸ë¡œìš´ ë‹¨ì–´ì˜ ì„ë² ë”©ì„ ì‹œê°í™”í•˜ì„¸ìš”.
5. ì´ëŸ¬í•œ í‘œí˜„ ë°©ë²•ë“¤ ê°„ì˜ ì°¨ì´ì ê³¼ ì´ë“¤ì´ í›„ì† NLP ì‘ì—…ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ”ì§€ ìƒê°í•´ ë³´ì„¸ìš”.

```python
# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
```

ì´ ì—°ìŠµì€ ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ìˆ ê³¼ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ í‘œí˜„ ë°©ë²•ì— ëŒ€í•œ ì‹¤ì œì ì¸ ê²½í—˜ì„ ì œê³µí•  ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‹¤ì œ NLP ì‘ì—…ì—ì„œ ì´ë“¤ì˜ ì‹¤ì§ˆì ì¸ ì˜í–¥ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤.
