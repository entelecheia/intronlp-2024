# ì„¸ì…˜ 2 - ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í‘œí˜„

## ì†Œê°œ

ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê¸°ìˆ ì„ ì‚´í´ë³´ê³  ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤. 1íšŒì°¨ì—ì„œ ë‹¤ë£¬ ê¸°ë³¸ ê°œë…ì„ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•œ ë” ì •êµí•œ ì ‘ê·¼ ë°©ì‹ì„ ì†Œê°œí•  ê²ƒì…ë‹ˆë‹¤.

ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê² ìŠµë‹ˆë‹¤:

```python
import re
import nltk
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.chunk import ne_chunk
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec
import spacy

# í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# spaCy ëª¨ë¸ ë¡œë“œ
nlp = spacy.load("en_core_web_sm")
```

## 1. ê³ ê¸‰ í…ìŠ¤íŠ¸ ì •ì œ ê¸°ìˆ 

1íšŒì°¨ì˜ ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì •ì œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ëª‡ ê°€ì§€ ê³ ê¸‰ ê¸°ìˆ ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

### ì´ëª¨ì§€ì™€ ì´ëª¨í‹°ì½˜ ì²˜ë¦¬

ì´ëª¨ì§€ì™€ ì´ëª¨í‹°ì½˜ì€ ê°ì • ì •ë³´ë¥¼ ë‹´ê³  ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì„ ì œê±°í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import emoji

def handle_emojis(text):
    # ì´ëª¨ì§€ë¥¼ í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´
    return emoji.demojize(text)

# ì‚¬ìš© ì˜ˆì‹œ
sample_text = "ì´ ì˜í™” ì •ë§ ì¢‹ì•„ìš”! ğŸ˜ğŸ‘"
processed_text = handle_emojis(sample_text)
print("ì›ë³¸ í…ìŠ¤íŠ¸:", sample_text)
print("ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸:", processed_text)
```

### ë¹„ ASCII ë¬¸ì ì²˜ë¦¬

ë¹„ ASCII ë¬¸ìë¥¼ í¬í•¨í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ê²½ìš°, ì´ë¥¼ ì œê±°í•˜ê±°ë‚˜ ì •ê·œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
import unicodedata

def normalize_unicode(text):
    # ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ê·œí™”
    return unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')

# ì‚¬ìš© ì˜ˆì‹œ
sample_text = "ì¹´í˜ ì˜¤ ë ˆëŠ” ë§›ìˆëŠ” ìŒë£Œì…ë‹ˆë‹¤"
normalized_text = normalize_unicode(sample_text)
print("ì›ë³¸ í…ìŠ¤íŠ¸:", sample_text)
print("ì •ê·œí™”ëœ í…ìŠ¤íŠ¸:", normalized_text)
```

## 2. ì¶•ì•½ì–´ì™€ íŠ¹ìˆ˜ ê²½ìš° ì²˜ë¦¬

ì¶•ì•½ì–´(ì˜ˆ: "don't", "I'm")ëŠ” ì¼ë¶€ NLP ì‘ì—…ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶•ì•½ì–´ë¥¼ í™•ì¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
CONTRACTION_MAP = {
    "ain't": "is not",
    "aren't": "are not",
    "can't": "cannot",
    "don't": "do not",
    "I'm": "I am",
    "isn't": "is not",
    "it's": "it is",
    "won't": "will not",
    # í•„ìš”ì— ë”°ë¼ ë” ë§ì€ ì¶•ì•½ì–´ ì¶”ê°€
}

def expand_contractions(text):
    for contraction, expansion in CONTRACTION_MAP.items():
        text = re.sub(r'\b' + contraction + r'\b', expansion, text, flags=re.IGNORECASE)
    return text

# ì‚¬ìš© ì˜ˆì‹œ
sample_text = "I can't believe it's not butter!"
expanded_text = expand_contractions(sample_text)
print("ì›ë³¸ í…ìŠ¤íŠ¸:", sample_text)
print("í™•ì¥ëœ í…ìŠ¤íŠ¸:", expanded_text)
```

## 3. ê°œì²´ëª… ì¸ì‹ (NER)

ê°œì²´ëª… ì¸ì‹ì€ í…ìŠ¤íŠ¸ì—ì„œ ëª…ëª…ëœ ê°œì²´(ì˜ˆ: ì¸ëª…, ì¡°ì§, ì¥ì†Œ)ë¥¼ ì‹ë³„í•˜ê³  ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. spaCyë¥¼ ì‚¬ìš©í•˜ì—¬ NERì„ ìˆ˜í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
def perform_ner(text):
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

# ì‚¬ìš© ì˜ˆì‹œ
sample_text = "ì• í”Œì€ ìŠ¤í‹°ë¸Œ ì¡ìŠ¤ê°€ ìº˜ë¦¬í¬ë‹ˆì•„ ì¿ í¼í‹°ë…¸ì—ì„œ ì„¤ë¦½í–ˆìŠµë‹ˆë‹¤."
entities = perform_ner(sample_text)
print("ëª…ëª…ëœ ê°œì²´:", entities)

# NER ê²°ê³¼ ì‹œê°í™”
from spacy import displacy
displacy.render(nlp(sample_text), style="ent", jupyter=True)
```

## 4. í’ˆì‚¬ íƒœê¹… (POS Tagging)

í’ˆì‚¬ íƒœê¹…ì€ ë‹¨ì–´ì— ë¬¸ë²•ì  ë²”ì£¼(ì˜ˆ: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)ë¥¼ ë¼ë²¨ë§í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ì‘ì—…ì— NLTKë¥¼ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```python
def pos_tag_text(text):
    words = word_tokenize(text)
    pos_tags = pos_tag(words)
    return pos_tags

# ì‚¬ìš© ì˜ˆì‹œ
sample_text = "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤."
pos_tags = pos_tag_text(sample_text)
print("í’ˆì‚¬ íƒœê·¸:", pos_tags)

# í’ˆì‚¬ íƒœê·¸ ì‹œê°í™”
def plot_pos_tags(pos_tags):
    words, tags = zip(*pos_tags)
    plt.figure(figsize=(12, 6))
    sns.barplot(x=list(words), y=[1]*len(words), hue=list(tags), dodge=False)
    plt.title("í’ˆì‚¬ íƒœê·¸")
    plt.xlabel("ë‹¨ì–´")
    plt.ylabel("")
    plt.legend(title="í’ˆì‚¬ íƒœê·¸", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

plot_pos_tags(pos_tags)
```

## 5. í…ìŠ¤íŠ¸ í‘œí˜„ ë°©ë²•

ì´ì œ ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ìˆ ì„ ë‹¤ë¤˜ìœ¼ë‹ˆ, ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### ë‹¨ì–´ ì£¼ë¨¸ë‹ˆ (Bag of Words, BoW)

ë‹¨ì–´ ì£¼ë¨¸ë‹ˆ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë¹ˆë„ì˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ë©°, ë¬¸ë²•ê³¼ ë‹¨ì–´ ìˆœì„œë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.

```python
def create_bow(corpus):
    vectorizer = CountVectorizer()
    bow_matrix = vectorizer.fit_transform(corpus)
    return bow_matrix, vectorizer.get_feature_names_out()

# ì‚¬ìš© ì˜ˆì‹œ
corpus = [
    "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ê°€ ê²Œìœ¼ë¥¸ ê°œë¥¼ ë›°ì–´ë„˜ìŠµë‹ˆë‹¤.",
    "ê²Œìœ¼ë¥¸ ê°œëŠ” í•˜ë£¨ ì¢…ì¼ ì ì„ ì¡ë‹ˆë‹¤.",
    "ë¹ ë¥¸ ê°ˆìƒ‰ ì—¬ìš°ëŠ” ë¹ ë¦…ë‹ˆë‹¤."
]

bow_matrix, feature_names = create_bow(corpus)
print("BoW íŠ¹ì„± ì´ë¦„:", feature_names)
print("BoW í–‰ë ¬ í˜•íƒœ:", bow_matrix.shape)
print("BoW í–‰ë ¬:")
print(bow_matrix.toarray())

# BoW ì‹œê°í™”
plt.figure(figsize=(12, 6))
sns.heatmap(bow_matrix.toarray(), annot=True, fmt='d', cmap='YlGnBu', xticklabels=feature_names)
plt.title("ë‹¨ì–´ ì£¼ë¨¸ë‹ˆ í‘œí˜„")
plt.ylabel("ë¬¸ì„œ")
plt.xlabel("ë‹¨ì–´")
plt.tight_layout()
plt.show()
```

### TF-IDF (ë‹¨ì–´ ë¹ˆë„-ì—­ë¬¸ì„œ ë¹ˆë„)

TF-IDFëŠ” ë¬¸ì„œ ì§‘í•©ì— ëŒ€í•œ ë¬¸ì„œ ë‚´ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

```python
def create_tfidf(corpus):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(corpus)
    return tfidf_matrix, vectorizer.get_feature_names_out()

# ì‚¬ìš© ì˜ˆì‹œ
tfidf_matrix, feature_names = create_tfidf(corpus)
print("TF-IDF íŠ¹ì„± ì´ë¦„:", feature_names)
print("TF-IDF í–‰ë ¬ í˜•íƒœ:", tfidf_matrix.shape)
print("TF-IDF í–‰ë ¬:")
print(tfidf_matrix.toarray())

# TF-IDF ì‹œê°í™”
plt.figure(figsize=(12, 6))
sns.heatmap(tfidf_matrix.toarray(), annot=True, fmt='.2f', cmap='YlGnBu', xticklabels=feature_names)
plt.title("TF-IDF í‘œí˜„")
plt.ylabel("ë¬¸ì„œ")
plt.xlabel("ë‹¨ì–´")
plt.tight_layout()
plt.show()
```

## 6. ë‹¨ì–´ ì„ë² ë”©

ë‹¨ì–´ ì„ë² ë”©ì€ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í¬ì°©í•˜ëŠ” ë‹¨ì–´ì˜ ë°€ì§‘ ë²¡í„° í‘œí˜„ì…ë‹ˆë‹¤. ì¸ê¸° ìˆëŠ” ë‹¨ì–´ ì„ë² ë”© ê¸°ìˆ ì¸ Word2Vecì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

```python
def train_word2vec(sentences, vector_size=100, window=5, min_count=1):
    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count)
    return model

# ì‚¬ìš© ì˜ˆì‹œ
tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]
w2v_model = train_word2vec(tokenized_corpus)

# t-SNEë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ì„ë² ë”© ì‹œê°í™”
from sklearn.manifold import TSNE

def plot_word_embeddings(model, words):
    word_vectors = [model.wv[word] for word in words]
    tsne = TSNE(n_components=2, random_state=42)
    embedded = tsne.fit_transform(word_vectors)

    plt.figure(figsize=(12, 8))
    for i, word in enumerate(words):
        x, y = embedded[i]
        plt.scatter(x, y)
        plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')
    plt.title("ë‹¨ì–´ ì„ë² ë”© ì‹œê°í™”")
    plt.xlabel("t-SNE íŠ¹ì„± 0")
    plt.ylabel("t-SNE íŠ¹ì„± 1")
    plt.tight_layout()
    plt.show()

plot_word_embeddings(w2v_model, ['ë¹ ë¥¸', 'ê°ˆìƒ‰', 'ì—¬ìš°', 'ê²Œìœ¼ë¥¸', 'ê°œ', 'ë›°ì–´ë„˜ìŠµë‹ˆë‹¤', 'ì ì„'])
```

## 7. ê²°ë¡  ë° ëª¨ë²” ì‚¬ë¡€

ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” ê³ ê¸‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê¸°ìˆ ê³¼ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ëª…ì‹¬í•´ì•¼ í•  ëª‡ ê°€ì§€ ëª¨ë²” ì‚¬ë¡€ì…ë‹ˆë‹¤:

1. íŠ¹ì • ì‘ì—…ê³¼ ë°ì´í„°ì…‹ì— ê¸°ë°˜í•˜ì—¬ ì „ì²˜ë¦¬ ê¸°ìˆ ì„ ì„ íƒí•˜ì„¸ìš”.
2. ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ í‘œí˜„ ë°©ë²• ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”:
   - BoWì™€ TF-IDFëŠ” ê°„ë‹¨í•˜ì§€ë§Œ ì˜ë¯¸ì  ì •ë³´ë¥¼ ìƒì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   - ë‹¨ì–´ ì„ë² ë”©ì€ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í¬ì°©í•˜ì§€ë§Œ íš¨ê³¼ì ìœ¼ë¡œ í›ˆë ¨í•˜ë ¤ë©´ ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.
3. ë‹¨ì–´ ì„ë² ë”©ì„ ì‚¬ìš©í•  ë•ŒëŠ” íŠ¹íˆ ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.
4. íŠ¹íˆ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ë‹¤ë£° ë•Œ ë‹¤ì–‘í•œ í‘œí˜„ ë°©ë²•ì— í•„ìš”í•œ ê³„ì‚° ë¦¬ì†ŒìŠ¤ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.
5. ì „ì²˜ë¦¬ ë° í‘œí˜„ ì„ íƒì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ê¸°ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì™€ í‘œí˜„ì€ NLP íŒŒì´í”„ë¼ì¸ì—ì„œ ì¤‘ìš”í•œ ë‹¨ê³„ì´ë©°, ì—¬ê¸°ì„œ ë‚´ë¦¬ëŠ” ì„ íƒì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒì„ ê¸°ì–µí•˜ì„¸ìš”.

## ì—°ìŠµ

1. ì„ íƒí•œ ë°ì´í„°ì…‹(ì˜ˆ: ë‰´ìŠ¤ ê¸°ì‚¬ ëª¨ìŒ, íŠ¸ìœ—, ì œí’ˆ ë¦¬ë·°)ì„ ì„ íƒí•˜ì„¸ìš”.
2. ìš°ë¦¬ê°€ ë°°ìš´ ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ìˆ (ì˜ˆ: ì´ëª¨ì§€ ì²˜ë¦¬, ì¶•ì•½ì–´ í™•ì¥, NER)ì„ ì ìš©í•˜ì„¸ìš”.
3. ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì˜ BoWì™€ TF-IDF í‘œí˜„ì„ ëª¨ë‘ ë§Œë“œì„¸ìš”.
4. ë°ì´í„°ì…‹ì—ì„œ Word2Vec ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ëª‡ ê°€ì§€ í¥ë¯¸ë¡œìš´ ë‹¨ì–´ì˜ ì„ë² ë”©ì„ ì‹œê°í™”í•˜ì„¸ìš”.
5. ì´ëŸ¬í•œ í‘œí˜„ ë°©ë²•ë“¤ ê°„ì˜ ì°¨ì´ì ê³¼ ì´ë“¤ì´ í›„ì† NLP ì‘ì—…ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ”ì§€ ìƒê°í•´ ë³´ì„¸ìš”.

```python
# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
```

ì´ ì—°ìŠµì€ ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ìˆ ê³¼ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ í‘œí˜„ ë°©ë²•ì— ëŒ€í•œ ì‹¤ì œì ì¸ ê²½í—˜ì„ ì œê³µí•  ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‹¤ì œ NLP ì‘ì—…ì—ì„œ ì´ë“¤ì˜ ì‹¤ì§ˆì ì¸ ì˜í–¥ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤.
