# 10주차 세션 1: LLM 기반 Q&A 시스템 소개

## 개요

이번 세션에서는 대규모 언어 모델(LLM)을 사용한 질의응답(Q&A) 시스템 구축의 기초를 살펴보겠습니다. 현대 Q&A 시스템의 아키텍처를 살펴보고, 전통적인 접근 방식과의 차이점을 강조할 것입니다. 문서 처리 파이프라인, 벡터 저장소, LLM 통합과 같은 주요 구성 요소와 이러한 요소들이 효율적이고 지능적인 Q&A 솔루션을 만들기 위해 어떻게 상호 작용하는지 배우게 됩니다.

## 소개

LLM의 등장은 자연어 처리(NLP) 분야를 혁신하여, 기계가 놀라운 정확도로 인간과 같은 텍스트를 이해하고 생성할 수 있게 되었습니다. LLM으로 구동되는 Q&A 시스템은 방대한 양의 데이터를 활용하여 복잡한 쿼리를 이해하고 상세한 답변을 제공할 수 있습니다. 이 세션은 검색 증강 생성(RAG) 시스템과 같은 고급 애플리케이션을 개발하기 위한 기초를 마련하고 최종 프로젝트에 필요한 기술을 제공하므로 AI 엔지니어 지망생들에게 매우 중요합니다.

## LLM 기반 Q&A 시스템의 아키텍처

### 주요 구성 요소와 상호 작용

- **문서 처리 파이프라인**:

  - _정의_: 원시 문서를 수집하고 추가 분석을 위해 전처리하는 시스템.
  - _주요 사항_:
    - 다양한 형식(PDF, HTML 등)에서 텍스트 추출.
    - 텍스트 데이터 정제 및 정규화.
    - 문서를 관리 가능한 조각으로 분할.
  - _예시_: 연구 논문을 색인화를 위한 깨끗한 텍스트 조각으로 변환.

- **벡터 저장소와 검색**:

  - _정의_: 효율적인 유사도 검색을 위해 문서 임베딩을 벡터 데이터베이스에 저장하는 방법.
  - _주요 사항_:
    - 텍스트 데이터의 벡터 표현(임베딩) 생성.
    - Pinecone이나 Weaviate와 같은 데이터베이스에 임베딩 저장.
    - 쿼리 유사도를 기반으로 관련 문서를 빠르게 검색.
  - _예시_: 사용자가 질문할 때 데이터베이스에서 가장 관련성 높은 기사 검색.

- **LLM 통합**:

  - _정의_: 쿼리를 해석하고 응답을 생성하기 위한 LLM 통합.
  - _주요 사항_:
    - GPT-4와 같은 모델을 사용하여 자연어 쿼리 이해.
    - 일관성 있고 문맥에 맞는 답변 생성.
  - _예시_: OpenAI의 GPT-4 API를 사용하여 검색된 문서를 기반으로 답변 생성.

- **응답 생성**:
  - _정의_: 사용자에게 전달되는 최종 답변을 작성하는 과정.
  - _주요 사항_:
    - 문서에서 검색한 정보와 LLM 기능을 결합.
    - 사용자 친화적인 방식으로 응답 형식 지정.
  - _예시_: 사용자의 질문에 직접적으로 답하는 간결한 요약 제시.

### 전통적인 Q&A 접근 방식과의 비교

- **전통적인 접근 방식**:

  - 키워드 매칭과 미리 정의된 규칙에 크게 의존.
  - 문맥과 뉘앙스에 대한 이해가 제한적.
  - FAQ 시스템과 기본 검색 엔진이 예시.

- **LLM 기반 접근 방식**:
  - 문맥을 이해하고 복잡한 대화형 쿼리를 처리할 수 있음.
  - 자연어 응답 생성.
  - 명시적인 재프로그래밍 없이 다양한 주제에 적응 가능.

### 구성 요소 간 상호 작용

- **문서 처리 파이프라인**은 **벡터 저장소**를 위한 데이터를 준비하여 효율적인 검색을 가능하게 함.
- **LLM**은 사용자의 쿼리와 검색된 문서를 모두 사용하여 정확한 응답을 생성.
- **응답 생성**은 일관성 있고 관련성 있는 최종 답변을 전달.

## 문서 처리 및 구문 분석

### 문서 전처리 기법

#### 텍스트 추출 및 정제

- **정의**: 원시 문서를 깨끗하고 기계가 읽을 수 있는 텍스트로 변환하는 과정.
- **주요 사항**:
  - 불필요한 서식, 기호, 메타데이터 제거.
  - 텍스트 정규화(예: 소문자화, 불용어 제거).
- **예시**:
  - `PyPDF2` 라이브러리를 사용하여 PDF에서 텍스트를 추출하고 불필요한 공백 제거.

#### 청킹 전략

- **정의**: 큰 텍스트를 작고 일관된 섹션으로 나누기.
- **주요 사항**:
  - 처리 효율성 향상.
  - 더 정확한 정보 검색에 도움.
- **예시**:
  - 교과서 장을 상세한 색인화를 위해 단락이나 섹션으로 분할.

#### 메타데이터 추출

- **정의**: 문서에 대한 추가 정보 캡처.
- **주요 사항**:
  - 저자, 발행일, 키워드 포함.
  - 검색 및 검색 기능 향상.
- **예시**:
  - 응답에 문맥을 제공하기 위해 뉴스 기사에서 저자와 날짜 추출.

### 다양한 문서 형식 처리

#### 일반 텍스트

- **주요 사항**:
  - 가장 단순한 형식.
  - 최소한의 추출 노력 필요.
- **예시**:
  - `.txt` 파일을 처리를 위해 프로그램에 직접 읽기.

#### PDF 문서

- **주요 사항**:
  - 복잡한 레이아웃(열, 이미지) 포함 가능.
  - 추출을 위해 `PyPDF2` 또는 `pdfminer` 같은 라이브러리 필요.
- **예시**:
  - PDF 형식으로 제공된 학술 논문에서 텍스트 추출.

#### HTML 콘텐츠

- **주요 사항**:
  - 웹페이지에는 HTML 태그, 스크립트, 스타일이 포함.
  - `BeautifulSoup`을 사용하여 의미 있는 콘텐츠 파싱 및 추출.
- **예시**:
  - Q&A 시스템에 포함할 블로그 기사 스크래핑 및 정제.

#### 구조화된 데이터(JSON, CSV)

- **주요 사항**:
  - 데이터가 이미 구조화된 형식.
  - 데이터프레임이나 딕셔너리로 직접 파싱 가능.
- **예시**:
  - 제품 FAQ의 CSV 파일을 시스템으로 가져오기.

### 정보 추출 방법

#### 정규 표현식

- **정의**: 특정 텍스트 시퀀스를 매칭하고 추출하는 데 사용되는 패턴.
- **주요 사항**:
  - 패턴 매칭(이메일, 날짜)에 강력.
  - 패턴의 신중한 작성 필요.
- **예시**:
  - `\(\d{3}\) \d{3}-\d{4}` 같은 정규식 패턴을 사용하여 전화번호 추출.

#### 규칙 기반 파싱

- **정의**: 미리 정의된 규칙을 사용하여 정보를 식별하고 추출.
- **주요 사항**:
  - 구조화 및 반구조화된 데이터에 적합.
  - 비구조화된 데이터에는 덜 유연.
- **예시**:
  - 각 단락의 첫 문장을 요약으로 추출.

#### LLM 지원 추출

- **정의**: 문맥을 기반으로 정보를 이해하고 추출하기 위해 LLM 활용.
- **주요 사항**:
  - 비구조화된 데이터를 효과적으로 처리.
  - 명시적으로 언급되지 않은 정보 추론 가능.
- **예시**:
  - LLM에게 구절에서 논의된 주요 주제 식별 요청.

### 요약

효과적인 문서 처리 및 구문 분석 기술의 이해와 구현은 강력한 Q&A 시스템 구축에 필수적입니다. 다양한 형식의 데이터를 효율적으로 추출하고 준비함으로써, 우리 시스템이 더 정확하고 관련성 있는 답변을 제공할 수 있는 고품질 정보를 확보할 수 있습니다.

## 실습 예제와 연습

### 예제 1: HTML 페이지에서 텍스트 추출 및 정제

**목표**: `BeautifulSoup`을 사용하여 웹페이지에서 텍스트를 추출하고 정제.

**단계**:

1. **필요한 라이브러리 가져오기**:

   import requests
   from bs4 import BeautifulSoup

2. **웹페이지 콘텐츠 가져오기**:

   url = 'https://www.example.com'
   response = requests.get(url)
   html_content = response.text

3. **HTML 콘텐츠 파싱**:

   soup = BeautifulSoup(html_content, 'html.parser')

4. **텍스트 추출 및 정제**:

   for script in soup(["script", "style"]):
   script.decompose()
   text = soup.get_text(separator=' ')

5. **정제된 텍스트 출력 또는 저장**:

   print(text.strip())

### 예제 2: 큰 텍스트 문서 청킹

**목표**: 긴 텍스트를 색인화를 위한 작은 청크로 분할.

**단계**:

1. **텍스트 데이터 로드**:

   with open('large_document.txt', 'r') as file:
   text = file.read()

2. **청크 크기 정의** (예: 500단어):

   chunk_size = 500
   words = text.split()

3. **청크로 분할**:

   chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

4. **각 청크 처리**:

   for idx, chunk in enumerate(chunks): # 여기서 임베딩 또는 색인화 수행
   pass

### 연습

**과제**: 정규 표현식을 사용하여 주어진 텍스트에서 모든 날짜를 추출하는 스크립트 작성.

**지시사항**:

1. 다양한 날짜 형식을 포함하는 텍스트 준비 (예: "January 1, 2021", "01/01/2021", "2021-01-01").
2. 이러한 날짜 형식과 일치하는 정규 표현식 패턴 작성.
3. `re.findall()`을 사용하여 모든 날짜 추출.
4. 추출된 날짜
