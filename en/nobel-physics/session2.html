
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Session 2: Deep Learning Evolution and Advanced Neural Network Architectures &#8212; Introduction to NLP and LLMs 2024</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nobel-physics/session2';</script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Session 3: Insights from Interviews" href="session3.html" />
    <link rel="prev" title="Session 1: Foundational Discoveries in Machine Learning with Artificial Neural Networks" href="session1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction to NLP and LLMs 2024</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Home
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/session1.html">Week 1 Session 1: Foundations and Evolution of NLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week01/session2.html">Week 1 Session 2: The Revolution in Modern NLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week01/wk1-lab1.html">Week 1 Lab - Introduction to NLP Basics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week02/index.html">Week 2: Basics of Text Preprocessing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week02/session1.html">Week 2 Session 1: Text Preprocessing Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week02/session2.html">Week 2 Session 2: Advanced Text Preprocessing and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week02/session3.html">Week 2 Session 3: Korean Text Preprocessing and Tokenization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week03/index.html">Week 3: Fundamentals of Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week03/session1.html">Week 3 Session 1: Introduction to Language Models and N-grams</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week03/session2.html">Week 3 Session 2: Advanced Statistical Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week04/index.html">Week 4: Word Embeddings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week04/session1.html">Week 4 Session 1: Introduction to Word Embeddings and Word2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week04/session2.html">Week 4 Session 2:  Advanced Word Embeddings</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week05/index.html">Week 5: Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week05/session1.html">Week 5 Session 1: Introduction to Transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week05/session2.html">Week 5 Session 2: BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week05/session3.html">Week 5 Session 3: Practical Implementation and Visualization of Transformers</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week06/index.html">Week 6: Understanding LLM APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week06/session1.html">Week 6 Session 1: Large Language Model (LLM) Basics and Training Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week06/session2.html">Week 6 Session 2: Introduction to LLM APIs and OpenAI API Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week06/session3.html">Week 6 Session 3: Sampling Methods and Text Generation</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Special Lecture: 2024 Nobel Prize in Physics</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="session1.html">Session 1: Foundational Discoveries in Machine Learning with Artificial Neural Networks</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Session 2: Deep Learning Evolution and Advanced Neural Network Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="session3.html">Session 3: Insights from Interviews</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nobel-chemistry/index.html">Special Lecture: 2024 Nobel Prize in Chemistry</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../nobel-chemistry/session1.html">Session 1: Computational Protein Design and De Novo Protein Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nobel-chemistry/session2.html">Session 2: Protein Structure Prediction Using Artificial Intelligence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nobel-chemistry/session3.html">Session 3: Insights from Interviews</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week09/index.html">Week 9: Basics of Prompt Engineering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week09/session1.html">Week 9 Session 1: Introduction to Prompt Engineering and Core Techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week09/session2.html">Week 9 Session 2: Advanced Prompting Strategies and Prompt Design Principles</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week10/index.html">Week 10: Building LLM-based Q&amp;A Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week10/session1.html">Week 10 Session 1: Introduction to LLM-based Q&amp;A Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week10/session2.html">Week 10 Session 2: Vector Databases and Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week10/session3.html">Week 10 Session 3: System Integration and Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week11/index.html">Week 11: Web Application Development Basics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week11/session1.html">Week 11 Session 1: Introduction to Web Development and Flask</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week11/session2.html">Week 11 Session 2: Advanced Flask and Introduction to Streamlit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week11/session3.html">Week 11 Session 3: Integrating LLM APIs and Deployment</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week12/index.html">Week 12: Controlling and Structuring LLM Outputs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week12/session1.html">Week 12 Session 1: Fundamentals of LLM Output Structuring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week12/session2.html">Week 12 Session 2: Advanced LLM Output Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week12/session3.html">Week 12 Session 3: Real-World LLM Applications</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">Team Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../projects/proposal.html">NLP Project Proposal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../projects/research-note.html">Week [n] Project Research Note</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
<li class="toctree-l1"><a class="reference external" href="https://halla.ai">CHU AI Department</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/intronlp-2024" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/intronlp-2024/edit/main/book/en/nobel-physics/session2.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/intronlp-2024/issues/new?title=Issue%20on%20page%20%2Fnobel-physics/session2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nobel-physics/session2.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Session 2: Deep Learning Evolution and Advanced Neural Network Architectures</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rise-of-deep-learning">The Rise of Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-training-deep-networks">Challenges in Training Deep Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-gradient-problem-explained">Vanishing Gradient Problem Explained</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#breakthrough-with-restricted-boltzmann-machines-rbms">Breakthrough with Restricted Boltzmann Machines (RBMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-wise-pretraining">Layer-wise Pretraining</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geoffrey-hinton-s-key-contributions-to-neural-networks">Geoffrey Hinton’s Key Contributions to Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-1986">1. Backpropagation (1986)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-backpropagation-works">How Backpropagation Works</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-belief-nets-dbns-2006">2. Deep Belief Nets (DBNs) (2006)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-dbns">Understanding DBNs</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimension-reduction-using-rbms-2006">3. Dimension Reduction Using RBMs (2006)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dimension-reduction-explained">Dimension Reduction Explained</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet-2012">4. AlexNet (2012)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet-s-impact">AlexNet’s Impact</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-contrastive-learning-2020">5. Visual Contrastive Learning (2020)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-contrastive-learning">Understanding Contrastive Learning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-cnns-work">How CNNs Work</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-networks-and-lstms">Recurrent Neural Networks and LSTMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-rnns-and-lstms">Understanding RNNs and LSTMs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-networks">Transformer Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-transformers-work">How Transformers Work</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-future-directions">Applications and Future Directions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="session-2-deep-learning-evolution-and-advanced-neural-network-architectures">
<h1>Session 2: Deep Learning Evolution and Advanced Neural Network Architectures<a class="headerlink" href="#session-2-deep-learning-evolution-and-advanced-neural-network-architectures" title="Link to this heading">#</a></h1>
<p>This session explores the fascinating journey from foundational neural network models to modern deep learning architectures. We’ll focus on the pivotal contributions of Geoffrey Hinton, the development of convolutional neural networks (CNNs), and the groundbreaking introduction of transformer models. This evolution has revolutionized artificial intelligence, enabling machines to process and understand complex data in ways that were once thought impossible.</p>
<section id="the-rise-of-deep-learning">
<h2>The Rise of Deep Learning<a class="headerlink" href="#the-rise-of-deep-learning" title="Link to this heading">#</a></h2>
<section id="challenges-in-training-deep-networks">
<h3>Challenges in Training Deep Networks<a class="headerlink" href="#challenges-in-training-deep-networks" title="Link to this heading">#</a></h3>
<p>During the 1990s and early 2000s, researchers faced significant hurdles in training deep neural networks (DNNs). One of the most prominent issues was the <strong>vanishing gradient problem</strong>.</p>
<section id="vanishing-gradient-problem-explained">
<h4>Vanishing Gradient Problem Explained<a class="headerlink" href="#vanishing-gradient-problem-explained" title="Link to this heading">#</a></h4>
<p>Imagine you’re trying to pass a message through a long chain of people. As the message travels, it becomes distorted, and by the time it reaches the end, it might be unrecognizable. Similarly, in deep neural networks, as we add more layers, the gradients (which are like our “message”) used to update the weights become extremely small (or “vanish”) as they’re propagated backwards through the network. This makes it difficult for the earlier layers to learn effectively, as they receive very little meaningful feedback.</p>
</section>
</section>
<section id="breakthrough-with-restricted-boltzmann-machines-rbms">
<h3>Breakthrough with Restricted Boltzmann Machines (RBMs)<a class="headerlink" href="#breakthrough-with-restricted-boltzmann-machines-rbms" title="Link to this heading">#</a></h3>
<p>In 2006, Geoffrey Hinton introduced a novel approach to tackle these challenges using <strong>Restricted Boltzmann Machines (RBMs)</strong> and <strong>layer-wise pretraining</strong>.</p>
<section id="layer-wise-pretraining">
<h4>Layer-wise Pretraining<a class="headerlink" href="#layer-wise-pretraining" title="Link to this heading">#</a></h4>
<p>This technique involves training the network one layer at a time, starting from the input layer and moving towards the output layer. Each layer is initially trained as an RBM, which learns to reconstruct its input. After this pretraining phase, the entire network is fine-tuned using backpropagation.</p>
<p>Think of it like building a skyscraper: instead of trying to construct all floors simultaneously, we build and stabilize each floor before moving to the next. This approach helps in initializing the weights of the network in a better state, making the subsequent fine-tuning process more effective.</p>
</section>
</section>
</section>
<section id="geoffrey-hinton-s-key-contributions-to-neural-networks">
<h2>Geoffrey Hinton’s Key Contributions to Neural Networks<a class="headerlink" href="#geoffrey-hinton-s-key-contributions-to-neural-networks" title="Link to this heading">#</a></h2>
<p>Geoffrey Hinton, often referred to as the “Godfather of Deep Learning,” has made several groundbreaking contributions that have shaped the field of neural networks and artificial intelligence.</p>
<section id="backpropagation-1986">
<h3>1. Backpropagation (1986)<a class="headerlink" href="#backpropagation-1986" title="Link to this heading">#</a></h3>
<p>In 1986, Hinton, along with David Rumelhart and Ronald Williams, introduced the <strong>backpropagation algorithm</strong> for training multi-layer neural networks.</p>
<p><img alt="Backpropagation Image" src="../_images/Hinton-papers-1.jpg" />
<img alt="Backpropagation Image" src="../_images/Hinton-papers-2.jpg" /></p>
<section id="how-backpropagation-works">
<h4>How Backpropagation Works<a class="headerlink" href="#how-backpropagation-works" title="Link to this heading">#</a></h4>
<p>Imagine you’re adjusting the aim of a complex trebuchet to hit a target. You make a shot, see how far off you are, and then adjust each part of the machine accordingly. Backpropagation works similarly:</p>
<ol class="arabic simple">
<li><p>The network makes a prediction.</p></li>
<li><p>The error (difference between prediction and actual output) is calculated.</p></li>
<li><p>This error is then propagated backwards through the network.</p></li>
<li><p>Each weight is adjusted based on its contribution to the error.</p></li>
</ol>
<p>This process allows the network to learn from its mistakes and gradually improve its performance, much like how you’d fine-tune the trebuchet with each attempt.</p>
</section>
</section>
<section id="deep-belief-nets-dbns-2006">
<h3>2. Deep Belief Nets (DBNs) (2006)<a class="headerlink" href="#deep-belief-nets-dbns-2006" title="Link to this heading">#</a></h3>
<p>In 2006, Hinton introduced <strong>Deep Belief Networks (DBNs)</strong>, which use layer-wise pretraining to overcome the vanishing gradient problem.</p>
<p><img alt="Deep Belief Nets Image" src="../_images/Hinton-papers-3.jpg" /></p>
<section id="understanding-dbns">
<h4>Understanding DBNs<a class="headerlink" href="#understanding-dbns" title="Link to this heading">#</a></h4>
<p>Think of DBNs as a tower of building blocks, where each block is an RBM. The network is built from the bottom up, with each layer learning to represent the data it receives from the layer below. This approach allows the network to learn increasingly abstract features as you move up the layers.</p>
<p>For example, in image recognition:</p>
<ul class="simple">
<li><p>The first layer might learn to detect edges.</p></li>
<li><p>The second layer might combine these edges to recognize simple shapes.</p></li>
<li><p>Higher layers might recognize more complex patterns like faces or objects.</p></li>
</ul>
<p>This hierarchical learning makes DBNs particularly effective for complex pattern recognition tasks.</p>
</section>
</section>
<section id="dimension-reduction-using-rbms-2006">
<h3>3. Dimension Reduction Using RBMs (2006)<a class="headerlink" href="#dimension-reduction-using-rbms-2006" title="Link to this heading">#</a></h3>
<p>Also in 2006, Hinton and Ruslan Salakhutdinov demonstrated the use of RBMs for <strong>dimension reduction</strong>.</p>
<p><img alt="Dimension Reduction Image" src="../_images/Hinton-papers-4.jpg" /></p>
<section id="dimension-reduction-explained">
<h4>Dimension Reduction Explained<a class="headerlink" href="#dimension-reduction-explained" title="Link to this heading">#</a></h4>
<p>Imagine you have a large, detailed map of a city, but you need to represent it on a small piece of paper. You’d have to simplify it, keeping only the most important features. This is essentially what dimension reduction does with data.</p>
<p>RBMs can learn to compress high-dimensional data (like images) into a lower-dimensional representation that captures the most important features. This makes subsequent processing more efficient and can help in tasks like data compression, feature extraction, and even in generating new data samples.</p>
</section>
</section>
<section id="alexnet-2012">
<h3>4. AlexNet (2012)<a class="headerlink" href="#alexnet-2012" title="Link to this heading">#</a></h3>
<p>In 2012, Hinton, along with his students Alex Krizhevsky and Ilya Sutskever, developed AlexNet, a convolutional neural network that won the ImageNet competition and revolutionized computer vision.</p>
<p><img alt="AlexNet Image" src="../_images/Hinton-papers-5.jpg" /></p>
<section id="alexnet-s-impact">
<h4>AlexNet’s Impact<a class="headerlink" href="#alexnet-s-impact" title="Link to this heading">#</a></h4>
<p>AlexNet was a game-changer for several reasons:</p>
<ol class="arabic simple">
<li><p><strong>Deep Architecture</strong>: It used multiple convolutional layers, allowing it to learn complex features from images.</p></li>
<li><p><strong>GPU Acceleration</strong>: It leveraged GPU computing, enabling the training of much larger networks than before.</p></li>
<li><p><strong>Novel Techniques</strong>: It introduced techniques like ReLU activation and dropout, which are now standard in many neural networks.</p></li>
</ol>
<p>AlexNet’s success in the ImageNet competition (reducing the error rate from 26% to 15.3%) marked the beginning of the deep learning revolution in computer vision.</p>
</section>
</section>
<section id="visual-contrastive-learning-2020">
<h3>5. Visual Contrastive Learning (2020)<a class="headerlink" href="#visual-contrastive-learning-2020" title="Link to this heading">#</a></h3>
<p>In 2020, Hinton contributed to advancements in <strong>contrastive learning</strong> for visual tasks.</p>
<p><img alt="Visual Contrastive Learning Image" src="../_images/Hinton-papers-6.jpg" /></p>
<section id="understanding-contrastive-learning">
<h4>Understanding Contrastive Learning<a class="headerlink" href="#understanding-contrastive-learning" title="Link to this heading">#</a></h4>
<p>Contrastive learning is like teaching a child to recognize objects by showing them pairs of images and asking, “Are these the same object?” Over time, the child (or in this case, the neural network) learns to identify key features that distinguish different objects.</p>
<p>In the context of machine learning:</p>
<ol class="arabic simple">
<li><p>The network is shown different augmentations (like rotations or color changes) of the same image.</p></li>
<li><p>It learns to recognize that these augmentations represent the same underlying image.</p></li>
<li><p>This process helps the network learn robust visual representations without needing labeled data.</p></li>
</ol>
<p>This approach has been particularly useful in scenarios where labeled data is scarce or expensive to obtain.</p>
</section>
</section>
</section>
<section id="convolutional-neural-networks-cnns">
<h2>Convolutional Neural Networks (CNNs)<a class="headerlink" href="#convolutional-neural-networks-cnns" title="Link to this heading">#</a></h2>
<p>While Hinton’s work revitalized deep learning, Yann LeCun and others advanced a special class of neural networks called <strong>Convolutional Neural Networks (CNNs)</strong>.</p>
<section id="how-cnns-work">
<h3>How CNNs Work<a class="headerlink" href="#how-cnns-work" title="Link to this heading">#</a></h3>
<p>Imagine you’re looking at a large mural. You don’t take in the entire image at once; instead, your eyes scan across it, focusing on different parts. CNNs work similarly:</p>
<ol class="arabic simple">
<li><p><strong>Convolutional Layers</strong>: These act like a sliding window, scanning across the image and detecting features like edges, textures, and shapes.</p></li>
<li><p><strong>Pooling Layers</strong>: These summarize the features detected in a particular region, making the network more robust to small variations in position.</p></li>
<li><p><strong>Fully Connected Layers</strong>: These take the high-level features learned by the convolutional and pooling layers and use them to make the final classification.</p></li>
</ol>
<p>This architecture makes CNNs particularly effective for tasks like image classification, object detection, and even facial recognition.</p>
</section>
</section>
<section id="recurrent-neural-networks-and-lstms">
<h2>Recurrent Neural Networks and LSTMs<a class="headerlink" href="#recurrent-neural-networks-and-lstms" title="Link to this heading">#</a></h2>
<p>To handle sequential data like text or time series, researchers developed <strong>Recurrent Neural Networks (RNNs)</strong> and their more advanced variant, <strong>Long Short-Term Memory (LSTM) networks</strong>.</p>
<section id="understanding-rnns-and-lstms">
<h3>Understanding RNNs and LSTMs<a class="headerlink" href="#understanding-rnns-and-lstms" title="Link to this heading">#</a></h3>
<p>Think of an RNN as a network with a memory. When processing a sequence (like a sentence), it considers not just the current input, but also what it has seen before. However, basic RNNs struggle with long-term dependencies.</p>
<p>LSTMs, introduced by Sepp Hochreiter and Jürgen Schmidhuber, solve this problem by using a more sophisticated memory cell. They can selectively remember or forget information, allowing them to capture long-term dependencies more effectively.</p>
<p>For example, in language translation, an LSTM can remember the subject of a sentence even if it’s separated from its verb by many words, ensuring grammatical correctness in the translation.</p>
</section>
</section>
<section id="transformer-networks">
<h2>Transformer Networks<a class="headerlink" href="#transformer-networks" title="Link to this heading">#</a></h2>
<p>In 2017, Vaswani et al. introduced the <strong>transformer architecture</strong>, which has become the backbone of many state-of-the-art language models.</p>
<section id="how-transformers-work">
<h3>How Transformers Work<a class="headerlink" href="#how-transformers-work" title="Link to this heading">#</a></h3>
<p>Transformers use a mechanism called <strong>self-attention</strong>, which allows them to weigh the importance of different parts of the input when processing each element.</p>
<p>Imagine you’re at a party trying to follow multiple conversations. You pay attention to different speakers based on what’s relevant to you. Transformers work similarly:</p>
<ol class="arabic simple">
<li><p>For each word in a sentence, the transformer calculates how much attention to pay to every other word.</p></li>
<li><p>This allows it to capture complex relationships between words, even if they’re far apart in the sentence.</p></li>
<li><p>Unlike RNNs, transformers can process all words in parallel, making them much faster to train.</p></li>
</ol>
<p>Transformers have revolutionized NLP tasks like machine translation, text generation, and question answering. Models like BERT and GPT, which are based on the transformer architecture, have set new benchmarks in language understanding and generation.</p>
</section>
</section>
<section id="applications-and-future-directions">
<h2>Applications and Future Directions<a class="headerlink" href="#applications-and-future-directions" title="Link to this heading">#</a></h2>
<p>The advancements in neural network architectures have led to breakthrough applications across various domains:</p>
<ul class="simple">
<li><p><strong>Image and Speech Recognition</strong>: CNNs have enabled accurate object detection in images and real-time speech recognition.</p></li>
<li><p><strong>Natural Language Processing</strong>: Transformer-based models power advanced chatbots, machine translation systems, and even AI writing assistants.</p></li>
<li><p><strong>Healthcare</strong>: Deep learning models are being used for medical image analysis, drug discovery, and personalized medicine.</p></li>
<li><p><strong>Autonomous Vehicles</strong>: CNNs and other deep learning models are crucial for perception and decision-making in self-driving cars.</p></li>
</ul>
<p>As these technologies continue to evolve, we can expect to see even more sophisticated AI systems that can understand and interact with the world in increasingly human-like ways.</p>
</section>
<section id="key-takeaways">
<h2>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Layer-wise Training</strong>: This technique, pioneered by Hinton, was crucial in overcoming the challenges of training deep networks.</p></li>
<li><p><strong>Specialized Architectures</strong>: CNNs for visual tasks and transformers for sequential data have dramatically improved AI’s ability to process complex, real-world information.</p></li>
<li><p><strong>Scalability and Efficiency</strong>: Modern architectures like transformers have made it possible to train enormous models on vast datasets, pushing the boundaries of what AI can achieve.</p></li>
<li><p><strong>Interdisciplinary Impact</strong>: The evolution of neural networks has had far-reaching effects across numerous fields, from healthcare to autonomous systems.</p></li>
</ol>
<p>As we continue to refine these architectures and develop new ones, the potential applications of AI are bound to expand, promising exciting developments in the years to come.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/intronlp-2024",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nobel-physics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/intronlp-2024"        data-repo-id="R_kgDOMnmcQA"        data-category="General"        data-category-id="DIC_kwDOMnmcQM4Ch5PF"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="en"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="session1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Session 1: Foundational Discoveries in Machine Learning with Artificial Neural Networks</p>
      </div>
    </a>
    <a class="right-next"
       href="session3.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Session 3: Insights from Interviews</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rise-of-deep-learning">The Rise of Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-training-deep-networks">Challenges in Training Deep Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-gradient-problem-explained">Vanishing Gradient Problem Explained</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#breakthrough-with-restricted-boltzmann-machines-rbms">Breakthrough with Restricted Boltzmann Machines (RBMs)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-wise-pretraining">Layer-wise Pretraining</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#geoffrey-hinton-s-key-contributions-to-neural-networks">Geoffrey Hinton’s Key Contributions to Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-1986">1. Backpropagation (1986)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-backpropagation-works">How Backpropagation Works</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-belief-nets-dbns-2006">2. Deep Belief Nets (DBNs) (2006)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-dbns">Understanding DBNs</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimension-reduction-using-rbms-2006">3. Dimension Reduction Using RBMs (2006)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dimension-reduction-explained">Dimension Reduction Explained</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet-2012">4. AlexNet (2012)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet-s-impact">AlexNet’s Impact</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-contrastive-learning-2020">5. Visual Contrastive Learning (2020)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-contrastive-learning">Understanding Contrastive Learning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-cnns-work">How CNNs Work</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-neural-networks-and-lstms">Recurrent Neural Networks and LSTMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-rnns-and-lstms">Understanding RNNs and LSTMs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-networks">Transformer Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-transformers-work">How Transformers Work</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-future-directions">Applications and Future Directions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
