
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Week 6 Session 1: Large Language Model (LLM) Basics and Training Strategies &#8212; Introduction to NLP and LLMs 2024</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/language_selector.css?v=6a8ebae4" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/jquery.js?v=5d32c60e"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script src="../_static/js/hoverxref.js"></script>
    <script src="../_static/js/tooltipster.bundle.min.js"></script>
    <script src="../_static/js/micromodal.min.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-BQJE5V9RK2"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-BQJE5V9RK2');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week06/session1';</script>
    <script src="../_static/chat.js?v=f0de43d7"></script>
    <script src="../_static/language_switcher.js?v=730be77c"></script>
    <link rel="icon" href="https://assets.entelecheia.ai/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Week 6 Session 2: Introduction to LLM APIs and OpenAI API Usage" href="session2.html" />
    <link rel="prev" title="Week 6: Understanding LLM APIs" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
    <script src="/_static/language_switcher.js"></script>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-lang-select">  <nav class="menu">    <ul class="clearfix">      <li class="current-item">        <a href="#" class="clicker">          English <span class="arrow">&#9660;</span>        </a>        <ul class="sub-menu">                    <li><a href="#" onclick="switchLanguage('en'); return false;">English</a></li>          <li><a href="#" onclick="switchLanguage('ko'); return false;">한국어</a></li>        </ul>      </li>    </ul>  </nav></div>
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction to NLP and LLMs 2024</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Home
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week01/index.html">Week 1: Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week01/session1.html">Week 1 Session 1: Foundations and Evolution of NLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week01/session2.html">Week 1 Session 2: The Revolution in Modern NLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week01/wk1-lab1.html">Week 1 Lab - Introduction to NLP Basics</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week02/index.html">Week 2: Basics of Text Preprocessing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week02/session1.html">Week 2 Session 1: Text Preprocessing Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week02/session2.html">Week 2 Session 2: Advanced Text Preprocessing and Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week02/session3.html">Week 2 Session 3: Korean Text Preprocessing and Tokenization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week03/index.html">Week 3: Fundamentals of Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week03/session1.html">Week 3 Session 1: Introduction to Language Models and N-grams</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week03/session2.html">Week 3 Session 2: Advanced Statistical Language Models</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week04/index.html">Week 4: Word Embeddings</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week04/session1.html">Week 4 Session 1: Introduction to Word Embeddings and Word2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week04/session2.html">Week 4 Session 2:  Advanced Word Embeddings</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week05/index.html">Week 5: Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week05/session1.html">Week 5 Session 1: Introduction to Transformers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week05/session2.html">Week 5 Session 2: BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week05/session3.html">Week 5 Session 3: Practical Implementation and Visualization of Transformers</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Week 6: Understanding LLM APIs</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Week 6 Session 1: Large Language Model (LLM) Basics and Training Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="session2.html">Week 6 Session 2: Introduction to LLM APIs and OpenAI API Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="session3.html">Week 6 Session 3: Sampling Methods and Text Generation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nobel-physics/index.html">Special Lecture: 2024 Nobel Prize in Physics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../nobel-physics/session1.html">Session 1: Foundational Discoveries in Machine Learning with Artificial Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nobel-physics/session2.html">Session 2: Deep Learning Evolution and Advanced Neural Network Architectures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nobel-physics/session3.html">Session 3: Insights from Interviews</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nobel-chemistry/index.html">Special Lecture: 2024 Nobel Prize in Chemistry</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../nobel-chemistry/session1.html">Session 1: Computational Protein Design and De Novo Protein Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nobel-chemistry/session2.html">Session 2: Protein Structure Prediction Using Artificial Intelligence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nobel-chemistry/session3.html">Session 3: Insights from Interviews</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week09/index.html">Week 9: Basics of Prompt Engineering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week09/session1.html">Week 9 Session 1: Introduction to Prompt Engineering and Core Techniques</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week09/session2.html">Week 9 Session 2: Advanced Prompting Strategies and Prompt Design Principles</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week10/index.html">Week 10: Building LLM-based Q&amp;A Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week10/session1.html">Week 10 Session 1: Introduction to LLM-based Q&amp;A Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week10/session2.html">Week 10 Session 2: Vector Databases and Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week10/session3.html">Week 10 Session 3: System Integration and Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week11/index.html">Week 11: Web Application Development Basics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week11/session1.html">Week 11 Session 1: Introduction to Web Development and Flask</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week11/session2.html">Week 11 Session 2: Advanced Flask and Introduction to Streamlit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week11/session3.html">Week 11 Session 3: Integrating LLM APIs and Deployment</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../week12/index.html">Week 12: Controlling and Structuring LLM Outputs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../week12/session1.html">Week 12 Session 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week12/session2.html">Week 12 Session 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../week12/session3.html">Week 12 Session 3</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../projects/index.html">Team Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../projects/proposal.html">NLP Project Proposal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../projects/research-note.html">Week [n] Project Research Note</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../syllabus/index.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about/index.html">Who made this book?</a></li>
<li class="toctree-l1"><a class="reference external" href="https://halla.ai">CHU AI Department</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/entelecheia/intronlp-2024" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/intronlp-2024/edit/main/book/en/week06/session1.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/entelecheia/intronlp-2024/issues/new?title=Issue%20on%20page%20%2Fweek06/session1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week06/session1.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Week 6 Session 1: Large Language Model (LLM) Basics and Training Strategies</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-large-language-models">1. Introduction to Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-and-overview">Definition and Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-impact">Applications and Impact</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals-of-transformer-architecture">2. Fundamentals of Transformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-transformer-model">The Transformer Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-mechanism">Self-Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components">Key Components</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-strategies">3. Pretraining Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives-of-pretraining">Objectives of Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-tasks">Pretraining Tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-and-challenges">Importance and Challenges</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning-sft">4. Supervised Fine-Tuning (SFT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-sft">What is SFT?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-and-techniques">Methods and Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-considerations">Practical Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-fine-tuning-peft">5. Parameter-Efficient Fine-Tuning (PEFT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-peft">Motivation for PEFT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#peft-techniques">PEFT Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#adapters">Adapters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prefix-tuning">Prefix-Tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bitfit">BitFit</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-limitations">Advantages and Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alignment-techniques">6. Alignment Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-alignment">Importance of Alignment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-from-human-feedback-rlhf">Reinforcement Learning from Human Feedback (RLHF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constitutional-ai">Constitutional AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-and-safety-considerations">Ethical and Safety Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">7. Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">8. References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="week-6-session-1-large-language-model-llm-basics-and-training-strategies">
<h1>Week 6 Session 1: Large Language Model (LLM) Basics and Training Strategies<a class="headerlink" href="#week-6-session-1-large-language-model-llm-basics-and-training-strategies" title="Link to this heading">#</a></h1>
<section id="introduction-to-large-language-models">
<h2>1. Introduction to Large Language Models<a class="headerlink" href="#introduction-to-large-language-models" title="Link to this heading">#</a></h2>
<section id="definition-and-overview">
<h3>Definition and Overview<a class="headerlink" href="#definition-and-overview" title="Link to this heading">#</a></h3>
<p>Large Language Models (LLMs) are neural network models with a vast number of parameters, typically in the order of billions or even trillions. They are trained on extensive corpora of text data to understand and generate human-like language. Examples include OpenAI’s GPT-3 and GPT-4, Google’s BERT, and Meta’s LLaMA.</p>
</section>
<section id="applications-and-impact">
<h3>Applications and Impact<a class="headerlink" href="#applications-and-impact" title="Link to this heading">#</a></h3>
<p>LLMs have revolutionized natural language processing (NLP) by achieving state-of-the-art results in various tasks:</p>
<ul class="simple">
<li><p><strong>Text Generation:</strong> Crafting human-like text for chatbots, storytelling, and content creation.</p></li>
<li><p><strong>Translation:</strong> Converting text from one language to another with high accuracy.</p></li>
<li><p><strong>Summarization:</strong> Condensing large documents into concise summaries.</p></li>
<li><p><strong>Question Answering:</strong> Providing answers to queries based on context.</p></li>
<li><p><strong>Sentiment Analysis:</strong> Determining the sentiment expressed in text data.</p></li>
</ul>
<p>The impact of LLMs extends to industries like healthcare, finance, education, and more, enabling advanced analytics and automation.</p>
</section>
</section>
<hr class="docutils" />
<section id="fundamentals-of-transformer-architecture">
<h2>2. Fundamentals of Transformer Architecture<a class="headerlink" href="#fundamentals-of-transformer-architecture" title="Link to this heading">#</a></h2>
<section id="the-transformer-model">
<h3>The Transformer Model<a class="headerlink" href="#the-transformer-model" title="Link to this heading">#</a></h3>
<p>Introduced by Vaswani et al. in 2017, the Transformer architecture has become the foundation for most LLMs. It addresses the limitations of recurrent neural networks (RNNs) by allowing for parallelization and better handling of long-range dependencies.</p>
</section>
<section id="self-attention-mechanism">
<h3>Self-Attention Mechanism<a class="headerlink" href="#self-attention-mechanism" title="Link to this heading">#</a></h3>
<p>At the core of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sequence relative to each other.</p>
<ul class="simple">
<li><p><strong>Key Components:</strong></p>
<ul>
<li><p><strong>Queries (Q):</strong> The vector representation of the current word.</p></li>
<li><p><strong>Keys (K):</strong> The vector representations of all words in the sequence.</p></li>
<li><p><strong>Values (V):</strong> The same as the keys but can be different based on implementation.</p></li>
</ul>
</li>
</ul>
<p>The attention score is computed as:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</div>
<p>where <span class="math notranslate nohighlight">\( d_k \)</span> is the dimension of the key vectors.</p>
</section>
<section id="key-components">
<h3>Key Components<a class="headerlink" href="#key-components" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Multi-Head Attention:</strong> Improves the model’s ability to focus on different positions.</p></li>
<li><p><strong>Feed-Forward Networks:</strong> Applies non-linear transformations to the attention outputs.</p></li>
<li><p><strong>Positional Encoding:</strong> Adds information about the position of words in the sequence.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="pretraining-strategies">
<h2>3. Pretraining Strategies<a class="headerlink" href="#pretraining-strategies" title="Link to this heading">#</a></h2>
<section id="objectives-of-pretraining">
<h3>Objectives of Pretraining<a class="headerlink" href="#objectives-of-pretraining" title="Link to this heading">#</a></h3>
<p>Pretraining aims to initialize the model’s parameters by learning from large unlabeled text corpora. This phase equips the model with a general understanding of language.</p>
</section>
<section id="pretraining-tasks">
<h3>Pretraining Tasks<a class="headerlink" href="#pretraining-tasks" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Masked Language Modeling (MLM):</strong> Used in models like BERT.</p>
<ul class="simple">
<li><p>Randomly masks tokens in the input and tasks the model with predicting them.</p></li>
</ul>
</li>
<li><p><strong>Causal Language Modeling (CLM):</strong> Used in models like GPT.</p>
<ul class="simple">
<li><p>Predicts the next word in a sequence, training the model to generate coherent text.</p></li>
</ul>
</li>
<li><p><strong>Seq2Seq Modeling:</strong> Combines encoding and decoding processes for tasks like translation.</p></li>
</ol>
</section>
<section id="importance-and-challenges">
<h3>Importance and Challenges<a class="headerlink" href="#importance-and-challenges" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Importance:</strong></p>
<ul>
<li><p>Captures syntax and semantics.</p></li>
<li><p>Provides a strong foundation for downstream tasks.</p></li>
</ul>
</li>
<li><p><strong>Challenges:</strong></p>
<ul>
<li><p>Computationally intensive.</p></li>
<li><p>Requires careful consideration of data quality and diversity.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="supervised-fine-tuning-sft">
<h2>4. Supervised Fine-Tuning (SFT)<a class="headerlink" href="#supervised-fine-tuning-sft" title="Link to this heading">#</a></h2>
<section id="what-is-sft">
<h3>What is SFT?<a class="headerlink" href="#what-is-sft" title="Link to this heading">#</a></h3>
<p>Supervised Fine-Tuning involves training the pretrained model on a specific task with labeled data. It tailors the model to perform well on that task by adjusting the parameters further.</p>
</section>
<section id="methods-and-techniques">
<h3>Methods and Techniques<a class="headerlink" href="#methods-and-techniques" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Data Preparation:</strong> Collecting and labeling high-quality datasets relevant to the task.</p></li>
<li><p><strong>Training Procedure:</strong></p>
<ul>
<li><p>Use a smaller learning rate to avoid overwriting pretrained knowledge.</p></li>
<li><p>Employ regularization techniques to prevent overfitting.</p></li>
</ul>
</li>
<li><p><strong>Evaluation Metrics:</strong> Selecting appropriate metrics to assess performance, such as accuracy, F1-score, or BLEU score.</p></li>
</ul>
</section>
<section id="practical-considerations">
<h3>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Compute Resources:</strong> Fine-tuning large models can be resource-intensive.</p></li>
<li><p><strong>Overfitting Risks:</strong> Smaller datasets can lead to overfitting; data augmentation may help.</p></li>
<li><p><strong>Hyperparameter Tuning:</strong> Essential for optimal performance.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="parameter-efficient-fine-tuning-peft">
<h2>5. Parameter-Efficient Fine-Tuning (PEFT)<a class="headerlink" href="#parameter-efficient-fine-tuning-peft" title="Link to this heading">#</a></h2>
<section id="motivation-for-peft">
<h3>Motivation for PEFT<a class="headerlink" href="#motivation-for-peft" title="Link to this heading">#</a></h3>
<p>Fine-tuning all parameters of a large model is computationally expensive and storage-intensive. PEFT addresses this by adjusting only a small subset of parameters, making the process more efficient.</p>
</section>
<section id="peft-techniques">
<h3>PEFT Techniques<a class="headerlink" href="#peft-techniques" title="Link to this heading">#</a></h3>
<section id="adapters">
<h4>Adapters<a class="headerlink" href="#adapters" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Concept:</strong> Introduced by Houlsby et al., adapters are small neural networks inserted between layers of the pretrained model.</p></li>
<li><p><strong>Function:</strong> They adjust representations for specific tasks without altering the main model weights.</p></li>
<li><p><strong>Advantages:</strong> Reduces the number of trainable parameters.</p></li>
</ul>
</section>
<section id="low-rank-adaptation-lora">
<h4>Low-Rank Adaptation (LoRA)<a class="headerlink" href="#low-rank-adaptation-lora" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Concept:</strong> Decomposes weight updates into low-rank matrices.</p></li>
<li><p><strong>Function:</strong> Applies low-rank approximations to the weight updates during fine-tuning.</p></li>
<li><p><strong>Advantages:</strong> Significantly reduces memory footprint and computational cost.</p></li>
</ul>
</section>
<section id="prefix-tuning">
<h4>Prefix-Tuning<a class="headerlink" href="#prefix-tuning" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Concept:</strong> Keeps the model weights frozen and optimizes a sequence of prefix tokens.</p></li>
<li><p><strong>Function:</strong> The prefix tokens influence the model’s activations, steering it towards task-specific outputs.</p></li>
<li><p><strong>Advantages:</strong> Requires tuning only the prefix parameters.</p></li>
</ul>
</section>
<section id="bitfit">
<h4>BitFit<a class="headerlink" href="#bitfit" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Concept:</strong> Fine-tunes only the bias terms in the model’s layers.</p></li>
<li><p><strong>Function:</strong> Adjusts biases to adapt to new tasks while keeping the majority of weights unchanged.</p></li>
<li><p><strong>Advantages:</strong> Extremely parameter-efficient.</p></li>
</ul>
</section>
</section>
<section id="advantages-and-limitations">
<h3>Advantages and Limitations<a class="headerlink" href="#advantages-and-limitations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Advantages:</strong></p>
<ul>
<li><p>Reduces computational and storage requirements.</p></li>
<li><p>Enables quick adaptation to multiple tasks.</p></li>
</ul>
</li>
<li><p><strong>Limitations:</strong></p>
<ul>
<li><p>May not achieve the same performance as full fine-tuning.</p></li>
<li><p>Selection of which parameters to tune is crucial.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="alignment-techniques">
<h2>6. Alignment Techniques<a class="headerlink" href="#alignment-techniques" title="Link to this heading">#</a></h2>
<section id="importance-of-alignment">
<h3>Importance of Alignment<a class="headerlink" href="#importance-of-alignment" title="Link to this heading">#</a></h3>
<p>Aligning LLMs with human values and intentions is critical to ensure that the models behave responsibly and ethically. Misaligned models can produce harmful or biased outputs.</p>
</section>
<section id="reinforcement-learning-from-human-feedback-rlhf">
<h3>Reinforcement Learning from Human Feedback (RLHF)<a class="headerlink" href="#reinforcement-learning-from-human-feedback-rlhf" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Concept:</strong> Combines reinforcement learning with human feedback to fine-tune the model.</p></li>
<li><p><strong>Process:</strong></p>
<ol class="arabic simple">
<li><p><strong>Supervised Fine-Tuning:</strong> Initialize the model with human-annotated data.</p></li>
<li><p><strong>Reward Modeling:</strong> Train a reward model based on human preferences.</p></li>
<li><p><strong>Policy Optimization:</strong> Use reinforcement learning (e.g., Proximal Policy Optimization) to optimize the model according to the reward model.</p></li>
</ol>
</li>
<li><p><strong>Advantages:</strong> Aligns the model’s outputs with human preferences.</p></li>
</ul>
</section>
<section id="constitutional-ai">
<h3>Constitutional AI<a class="headerlink" href="#constitutional-ai" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Concept:</strong> Proposed by Anthropic, it involves training models to follow a set of principles or a “constitution.”</p></li>
<li><p><strong>Function:</strong> Models are fine-tuned to generate outputs that adhere to predefined ethical guidelines without human intervention during training.</p></li>
<li><p><strong>Advantages:</strong> Reduces the need for extensive human feedback.</p></li>
</ul>
</section>
<section id="ethical-and-safety-considerations">
<h3>Ethical and Safety Considerations<a class="headerlink" href="#ethical-and-safety-considerations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bias and Fairness:</strong> Ensuring the model does not perpetuate or amplify biases present in training data.</p></li>
<li><p><strong>Transparency:</strong> Making the model’s decision-making process interpretable.</p></li>
<li><p><strong>Accountability:</strong> Establishing mechanisms to address and rectify harmful outputs.</p></li>
<li><p><strong>Regulation Compliance:</strong> Adhering to legal standards like GDPR for data privacy.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="conclusion">
<h2>7. Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Large Language Models have transformed the field of NLP, enabling machines to understand and generate human-like text. Understanding the basics of LLMs, from their transformer architecture to various training strategies, is essential for leveraging their capabilities effectively.</p>
<ul class="simple">
<li><p><strong>Pretraining</strong> provides a foundational understanding of language.</p></li>
<li><p><strong>Supervised Fine-Tuning (SFT)</strong> adapts the model to specific tasks.</p></li>
<li><p><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> offers a resource-friendly alternative to full fine-tuning.</p></li>
<li><p><strong>Alignment Techniques</strong> ensure that models act in accordance with human values and ethics.</p></li>
</ul>
<p>By combining these strategies, we can develop LLMs that are not only powerful but also responsible and aligned with societal needs.</p>
</section>
<hr class="docutils" />
<section id="references">
<h2>8. References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Vaswani, A., et al. (2017). <a class="reference external" href="https://arxiv.org/abs/1706.03762">“Attention is All You Need.”</a></p></li>
<li><p>Devlin, J., et al. (2018). <a class="reference external" href="https://arxiv.org/abs/1810.04805">“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.”</a></p></li>
<li><p>Radford, A., et al. (2019). <a class="reference external" href="https://openai.com/blog/better-language-models/">“Language Models are Unsupervised Multitask Learners.”</a></p></li>
<li><p>Houlsby, N., et al. (2019). <a class="reference external" href="https://arxiv.org/abs/1902.00751">“Parameter-Efficient Transfer Learning for NLP.”</a></p></li>
<li><p>Hu, E. J., et al. (2021). <a class="reference external" href="https://arxiv.org/abs/2106.09685">“LoRA: Low-Rank Adaptation of Large Language Models.”</a></p></li>
<li><p>Ouyang, L., et al. (2022). <a class="reference external" href="https://arxiv.org/abs/2203.02155">“Training language models to follow instructions with human feedback.”</a></p></li>
<li><p>Bai, Y., et al. (2022). <a class="reference external" href="https://arxiv.org/abs/2204.05862">“Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.”</a></p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "entelecheia/intronlp-2024",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week06"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
    <div class="giscus"></div>
<script src="https://giscus.app/client.js"        data-repo="entelecheia/intronlp-2024"        data-repo-id="R_kgDOMnmcQA"        data-category="General"        data-category-id="DIC_kwDOMnmcQM4Ch5PF"        data-mapping="pathname"        data-strict="1"        data-reactions-enabled="1"        data-emit-metadata="1"        data-input-position="bottom"        data-theme="noborder_light"        data-lang="en"        data-loading="lazy"        crossorigin="anonymous"        async></script>
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Week 6: Understanding LLM APIs</p>
      </div>
    </a>
    <a class="right-next"
       href="session2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Week 6 Session 2: Introduction to LLM APIs and OpenAI API Usage</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-large-language-models">1. Introduction to Large Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-and-overview">Definition and Overview</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-impact">Applications and Impact</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals-of-transformer-architecture">2. Fundamentals of Transformer Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-transformer-model">The Transformer Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-mechanism">Self-Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components">Key Components</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-strategies">3. Pretraining Strategies</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives-of-pretraining">Objectives of Pretraining</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pretraining-tasks">Pretraining Tasks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-and-challenges">Importance and Challenges</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-fine-tuning-sft">4. Supervised Fine-Tuning (SFT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-sft">What is SFT?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-and-techniques">Methods and Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-considerations">Practical Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-fine-tuning-peft">5. Parameter-Efficient Fine-Tuning (PEFT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-peft">Motivation for PEFT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#peft-techniques">PEFT Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#adapters">Adapters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prefix-tuning">Prefix-Tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bitfit">BitFit</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-limitations">Advantages and Limitations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alignment-techniques">6. Alignment Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-alignment">Importance of Alignment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-from-human-feedback-rlhf">Reinforcement Learning from Human Feedback (RLHF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constitutional-ai">Constitutional AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ethical-and-safety-considerations">Ethical and Safety Considerations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">7. Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">8. References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Young Joon Lee
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
